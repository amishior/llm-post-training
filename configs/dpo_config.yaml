# Base & reference models
model_name_or_path: "Qwen/Qwen2.5-3B-Instruct"
ref_model_name_or_path: null    # null = use frozen copy of model_name_or_path

output_dir: "outputs/dpo-qwen2.5-3b"
train_file: "data/raw/dpo_train.jsonl"
eval_file: "data/raw/sft_eval.jsonl"

max_prompt_length: 512
max_answer_length: 512
max_train_samples: null

per_device_train_batch_size: 1
gradient_accumulation_steps: 16
num_train_epochs: 1

learning_rate: 5e-6
beta: 0.1                    # DPO beta
loss_type: "sigmoid"         # or "hinge"

logging_steps: 50
save_steps: 500
save_total_limit: 3

use_lora: true
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]

bf16: true
gradient_checkpointing: true
